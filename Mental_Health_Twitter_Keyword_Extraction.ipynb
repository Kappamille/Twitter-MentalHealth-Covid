{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df6d9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5771982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anxiété\n",
    "#Dépression\n",
    "#Suicide\n",
    "#Psychiatrie\n",
    "#Psychologie\n",
    "#Fatigue\n",
    "#Stress\n",
    "#Insomnie\n",
    "#Tristesse \n",
    "#Panique \n",
    "#Ennui\n",
    "\n",
    "#Heureu.x.se\n",
    "#Joie\n",
    "#Bonheur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84411bc5",
   "metadata": {},
   "source": [
    "## Anxiété"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f2e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr anxiété until:2021-04-22' > file1.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr anxiety until:2021-04-22' > file2.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr anxiétéé until:2021-04-22' > file3.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d38b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.read_json('file1.json', lines=True)#\n",
    "s2 = pd.read_json('file2.json', lines=True)#\n",
    "s3 = pd.read_json('file3.json', lines=True)\n",
    "\n",
    "ss = pd.concat([s1, s2, s3])\n",
    "\n",
    "ssn = ss.drop_duplicates(subset='id', keep='first')\n",
    "#print(ssn)\n",
    "ssn.to_csv('anxiety_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f915b28",
   "metadata": {},
   "source": [
    "## Depression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b392133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr dépression until:2021-04-22' > file1.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr depressif until:2021-04-22' > file2.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr depressive until:2021-04-22' > file3.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr depression until:2021-04-22' > file4.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr deppression until:2021-04-22' > file5.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr deprime until:2021-04-22' > file6.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr deprimée until:2021-04-22' > file7.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cc3766",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.read_json('file1.json', lines=True)#\n",
    "s2 = pd.read_json('file2.json', lines=True)#\n",
    "s3 = pd.read_json('file3.json', lines=True)\n",
    "s4 = pd.read_json('file4.json', lines=True)\n",
    "s5 = pd.read_json('file5.json', lines=True)\n",
    "s6 = pd.read_json('file6.json', lines=True)\n",
    "s7 = pd.read_json('file7.json', lines=True)\n",
    "\n",
    "ss = pd.concat([s1, s2, s3, s4, s5, s6, s7])\n",
    "\n",
    "ssn = ss.drop_duplicates(subset='id', keep='first')\n",
    "#print(ssn)\n",
    "ssn.to_csv('depression_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a24cb36",
   "metadata": {},
   "source": [
    "## Suicide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b237ab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr suicide until:2021-04-22' > file1.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr sucide until:2021-04-22' > file2.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr suicidee until:2021-04-22' > file3.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr sucidee until:2021-04-22' > file4.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0508923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.read_json('file1.json', lines=True)#\n",
    "s2 = pd.read_json('file2.json', lines=True)#\n",
    "s3 = pd.read_json('file3.json', lines=True)\n",
    "s4 = pd.read_json('file4.json', lines=True)\n",
    "\n",
    "\n",
    "ss = pd.concat([s1, s2, s3, s4])\n",
    "\n",
    "ssn = ss.drop_duplicates(subset='id', keep='first')\n",
    "#print(ssn)\n",
    "ssn.to_csv('suicide_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93225c41",
   "metadata": {},
   "source": [
    "## Psychiatrie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fda397",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr psychiatre until:2021-04-22' > file1.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr psychiatrie until:2021-04-22' > file2.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr psychiatrique until:2021-04-22' > file3.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr Psychiatry until:2021-04-22' > file4.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032cb51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.read_json('file1.json', lines=True)#\n",
    "s2 = pd.read_json('file2.json', lines=True)#\n",
    "s3 = pd.read_json('file3.json', lines=True)\n",
    "s4 = pd.read_json('file4.json', lines=True)\n",
    "\n",
    "\n",
    "ss = pd.concat([s1, s2, s3, s4])\n",
    "\n",
    "ssn = ss.drop_duplicates(subset='id', keep='first')\n",
    "#print(ssn)\n",
    "ssn.to_csv('psychiatrie_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f5137b",
   "metadata": {},
   "source": [
    "## Psychologie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1ae3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr psychologue until:2021-04-22' > file1.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr psychologie until:2021-04-22' > file2.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr psychologique until:2021-04-22' > file3.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr Psychology until:2021-04-22' > file4.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01798a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.read_json('file1.json', lines=True)#\n",
    "s2 = pd.read_json('file2.json', lines=True)#\n",
    "s3 = pd.read_json('file3.json', lines=True)\n",
    "s4 = pd.read_json('file4.json', lines=True)\n",
    "\n",
    "\n",
    "ss = pd.concat([s1, s2, s3, s4])\n",
    "\n",
    "ssn = ss.drop_duplicates(subset='id', keep='first')\n",
    "#print(ssn)\n",
    "ssn.to_csv('psychologie_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6b0025",
   "metadata": {},
   "source": [
    "## Fatigue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ffe62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr fatigue until:2021-04-22' > file1.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr fatiguee until:2021-04-22' > file2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f468149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.read_json('file1.json', lines=True)#\n",
    "s2 = pd.read_json('file2.json', lines=True)#\n",
    "\n",
    "\n",
    "ss = pd.concat([s1, s2])\n",
    "\n",
    "ssn = ss.drop_duplicates(subset='id', keep='first')\n",
    "#print(ssn)\n",
    "ssn.to_csv('fatigue_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b530115",
   "metadata": {},
   "source": [
    "## Stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c534a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr stress until:2021-04-22' > file1.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr stressé until:2021-04-22' > file2.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr stressee until:2021-04-22' > file3.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr stresser until:2021-04-22' > file4.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b5886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.read_json('file1.json', lines=True)#\n",
    "s2 = pd.read_json('file2.json', lines=True)#\n",
    "s3 = pd.read_json('file3.json', lines=True)\n",
    "s4 = pd.read_json('file4.json', lines=True)\n",
    "\n",
    "\n",
    "ss = pd.concat([s1, s2, s3, s4])\n",
    "\n",
    "ssn = ss.drop_duplicates(subset='id', keep='first')\n",
    "#print(ssn)\n",
    "ssn.to_csv('stress_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bf547f",
   "metadata": {},
   "source": [
    "## Insomnie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a8f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr insomnie until:2021-04-22' > file1.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr insomniaque until:2021-04-22' > file2.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr insomnia until:2021-04-22' > file3.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr insomnies until:2021-04-22' > file4.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33653366",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.read_json('file1.json', lines=True)#\n",
    "s2 = pd.read_json('file2.json', lines=True)#\n",
    "s3 = pd.read_json('file3.json', lines=True)\n",
    "s4 = pd.read_json('file4.json', lines=True)\n",
    "\n",
    "\n",
    "ss = pd.concat([s1, s2, s3, s4])\n",
    "\n",
    "ssn = ss.drop_duplicates(subset='id', keep='first')\n",
    "#print(ssn)\n",
    "ssn.to_csv('insomnie_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd546192",
   "metadata": {},
   "source": [
    "## Tristesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611bcf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr tristesse until:2021-04-22' > file1.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr triste until:2021-04-22' > file2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.read_json('file1.json', lines=True)#\n",
    "s2 = pd.read_json('file2.json', lines=True)#\n",
    "\n",
    "\n",
    "ss = pd.concat([s1, s2])\n",
    "\n",
    "ssn = ss.drop_duplicates(subset='id', keep='first')\n",
    "#print(ssn)\n",
    "ssn.to_csv('tristesse_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a98b37",
   "metadata": {},
   "source": [
    "## Panique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b44c96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr panique until:2021-04-22' > file1.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr paniquee until:2021-04-22' > file2.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr panic until:2021-04-22' > file3.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837b5095",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.read_json('file1.json', lines=True)#\n",
    "s2 = pd.read_json('file2.json', lines=True)#\n",
    "s3 = pd.read_json('file3.json', lines=True)\n",
    "\n",
    "ss = pd.concat([s1, s2, s3])\n",
    "\n",
    "ssn = ss.drop_duplicates(subset='id', keep='first')\n",
    "#print(ssn)\n",
    "ssn.to_csv('panique_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff13009",
   "metadata": {},
   "source": [
    "## Ennui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba5db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr ennui until:2021-04-22' > file1.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr ennuyé until:2021-04-22' > file2.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr ennuyee until:2021-04-22' > file3.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr ennuie until:2021-04-22' > file4.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr ennuyeux until:2021-04-22' > file5.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr ennuyeuse until:2021-04-22' > file6.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6256d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.read_json('file1.json', lines=True)#\n",
    "s2 = pd.read_json('file2.json', lines=True)#\n",
    "s3 = pd.read_json('file3.json', lines=True)\n",
    "s4 = pd.read_json('file4.json', lines=True)\n",
    "s5 = pd.read_json('file5.json', lines=True)\n",
    "s6 = pd.read_json('file6.json', lines=True)\n",
    "\n",
    "ss = pd.concat([s1, s2, s3, s4, s5, s6])\n",
    "\n",
    "ssn = ss.drop_duplicates(subset='id', keep='first')\n",
    "#print(ssn)\n",
    "ssn.to_csv('ennui_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767d53d3",
   "metadata": {},
   "source": [
    "## Heureux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d665e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr heureux until:2021-04-22' > file1.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr heureuse until:2021-04-22' > file2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05567a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.read_json('file1.json', lines=True)#\n",
    "s2 = pd.read_json('file2.json', lines=True)#\n",
    "\n",
    "\n",
    "ss = pd.concat([s1, s2])\n",
    "\n",
    "ssn = ss.drop_duplicates(subset='id', keep='first')\n",
    "#print(ssn)\n",
    "ssn.to_csv('heureux_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e7b036",
   "metadata": {},
   "source": [
    "## bonheur/joie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4447e539",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr bonheur until:2021-04-22' > file1.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr joyeux until:2021-04-22' > file2.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr joyeuse until:2021-04-22' > file1.json\n",
    "snscrape --jsonl --progress --max-results 2000000 --since 2019-01-01 twitter-search 'lang:fr joie until:2021-04-22' > file2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ca621",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.read_json('file1.json', lines=True)#\n",
    "s2 = pd.read_json('file2.json', lines=True)#\n",
    "s3 = pd.read_json('file3.json', lines=True)#\n",
    "s4 = pd.read_json('file4.json', lines=True)#\n",
    "\n",
    "\n",
    "ss = pd.concat([s1, s2, s3, s4])\n",
    "\n",
    "ssn = ss.drop_duplicates(subset='id', keep='first')\n",
    "#print(ssn)\n",
    "ssn.to_csv('bonheur_joie_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ed67c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
